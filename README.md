<a name="readme-top"></a>
<aside>
üë£ *Passo a passo:*

---
**Instala√ß√£o e primeiro contato:**

<details>
<summary>Instala√ß√£o Scrapy</summary>

  -  Criar um novo projeto com ambiente virtual
  -  Abrir terminal e instalar o Scrapy com o comando ‚Äúpip install Scrapy‚Äù
</details>

<details>
<summary>Criar um projeto de Spider</summary>

-  Terminal: ‚Äúscrapy startproject nomedoprojetoscraper‚Äù
</details>

<details>
<summary>Criar uma Spider</summary>

-  Navegar at√© a pasta ‚Äúspiders‚Äù do projeto Scrapy criado
-  Terminal: ‚Äúscrapy genspider spidername site.to.scrape.com‚Äù
</details>

<details>
<summary>Instalar ipython para facilitar os testes</summary>

-  Terminal: ‚Äúpip install ipython‚Äù
-  No arquivo scrapy.cfg adicionar ‚ÄúSHELL= ipython‚Äù em baixo de default
</details>

<details>
<summary>Usando o terminal Ipython</summary>

-  Terminal: ‚Äúscrapy shell‚Äù para ativar o terminal ipython
-  No terminal Ipython (chamaremos de ITerminal):  ‚Äúfetch('https://books.toscrape.com/')‚Äù
-  ITerminal: ‚Äúresponse‚Äù ‚Üê <200 https://books.toscrape.com/>
-  Selecionando um elemento Iterminal: ‚Äúresponse.css('article.product_pod')‚Äù
-  Sair do ITerminal usando: ‚Äúexit‚Äù

- Tipos de sele√ß√£o Ipython:
  -  Salvar em uma vari√°vel: ‚Äúbooks = response.css('article.product_pod')‚Äù
  -  Verificar tamanho da lista da vari√°vel: ‚Äúlen(books)‚Äù
  -  Obter um item da lista: ‚Äúbook = books[0]‚Äù
  -  Obter o texto de um elemento ‚Äúa‚Äù dentro de um elemento ‚Äúh3‚Äù: ‚Äúbook.css('h3 a::text').get()‚Äù
  -  Obter o texto de um elemento dentro de outro elemento pela Classe: ‚Äúbook.css('.product_price .price_color::text').get()‚Äù
  -  Obter o conte√∫do de um atributo de um elemento: ‚Äúbook.css('h3 a').attrib['href']‚Äù ou ‚Äúresponse.css('.next a::attr(href)').get()‚Äù
</details>

<details>
<summary>Alterando uma Spider [nomedaspider.py](http://nomedaspider.py) (‚Äùbookspider.py‚Äù)</summary>

-  Alterar o m√©todo parse que receber√° a response igual ao ITerminal [1.1](#subsecao-1-1)
</details>

<details>
<summary>Ativando e testando uma Spider pelo terminal</summary>

-  Navegar at√© a pasta do projeto Scrapy, nesse caso bookscraper
-  Terminal: ‚Äúscrapy crawl bookspider‚Äù  Nota: o item ‚Äú'item_scraped_count': 20‚Äù referencia a quantidade de itens raspados.
</details>

<details>
<summary>Percorrendo pr√≥ximas p√°ginas</summary>

-  Encontrar o elemento respons√°vel pelo link da pr√≥xima p√°gina, nesse caso, ‚Äúresponse.css('.next a::attr(href)').get()‚Äù
-  Adicionar l√≥gica para percorrer pr√≥ximas p√°ginas usando callback ‚Äúyield response.follow(next_page_url, callback=self.parse)‚Äù [1.2](#subsecao-1-2)
</details>
</aside>

<aside>
  
---
**Intermedi√°rio:**

<details>
<summary>Adicionando Coleta de p√°ginas dentro da p√°gina principal</summary>

- Adicionar coleta de cada url de uma p√°gina do livro
- Adicionar m√©todo para coletar informa√ß√µes de uma p√°gina usando callback ‚Äúyield response.follow(book_url, callback=self.parse_book_page)‚Äù [2](#subsecao-2)
</details>

<details>
<summary>Exportar os dados manualmente para um arquivo</summary>

- Terminal: ‚Äúscrapy crawl bookspider -O bookdata.csv‚Äù para criar em formato csv
- Terminal: ‚Äúscrapy crawl bookspider -O bookdata.json‚Äù para criar em formato json
- Terminal: ‚Äúscrapy crawl bookspider -o bookdata.json‚Äù para adicionar em formato json
</details>

<details>
<summary>Criando e populando um scrapy Item</summary>

- Criar uma classe para o item no arquivo items.py [3.](#subsecao-3)
- Popular a classe do livro [3.1](#subsecao-3-1)
</details>

<details>
<summary>Rodando uma pipeline para pr√©-processar um item</summary>

- Habilitar a spider no arquivo de configura√ß√µes da spider ‚Äúsettings.py‚Äù descomentando o objeto ITEM_PIPELINES = {}
  Nota: Em ‚ÄúITEM_PIPELINES = {"bookscraper.pipelines.BookscraperPipeline": 300, }‚Äù o n√∫mero 300 representa a prioridade, onde, quanto menor o valor mais ‚Äúcedo‚Äù a pipeline vai rodar.
- Alterar o m√©todo process_item do item no arquivo ‚Äúpipelines.py‚Äù [4.](#subsecao-4)
</details>

<details>
<summary>Salvando em um banco de dados</summary>

- Podemos adicionar no arquivo de configura√ß√µes da spider ‚Äúsettings.py‚Äù um objeto FEEDS = {'booksdata.json': {'format': 'json'}} para definir um formato padr√£o de sa√≠da quando rodarmos o comando ‚Äúscrapy crawl bookspider‚Äù que √© equivalente ao comando ‚Äúscrapy crawl bookspider -O cleandata.json‚Äù
- Podemos tamb√©m sobrescrever uma configura√ß√£o do arquivo settings.py dentro do arquivo bookspider.py (arquivo referente √† spider), basta adicionar  ‚Äúcustom_settings = { 'FEEDS': { 'booksdata.json': {'format': 'json'}, 'overwrite': True }}‚Äù ao c√≥digo
- Baixar e instalar SQLITE.
- Adicionar uma classe no arquivo pipelines.py para lidar com o banco [5.](#subsecao-5)
- Adicionar um item no objeto ITEM_PIPELINES no arquivo settings.py com: "bookscraper.pipelines.SaveToSQLitePipeline": 400,"
</details>

<details>
<summary>Evitando bloqueio de requisi√ß√£o de um site</summary>

- Criar m√©todo para adicionar header ao request [6.](#subsecao-6)
- Adicionar vari√°veis que o m√©todo necessitar [6.1](#subsecao-6-1)
- Adicionar middleware ao arquivo settings.py [6.2](#subsecao-6-2)
</details>

<details>
<summary>Adicionando proxy √†s requisi√ß√µes</summary>

- Adicionar configura√ß√£o de proxy ao arquivo settings.py [7.](#subsecao-7)
- Alternativa usando um servi√ßo pago [7.1](#subsecao-7-1)
- Alternativa usando scrapeops proxy api [7.2](#subsecao-7-2)
</details>

<details>
<summary>Rodando uma Spider na nuvem </summary> 
    
Criando um projeto Scrapy Cloud [8](#subsecao-8)

---
- Criar uma conta em ‚Äú[https://app.zyte.com](https://app.zyte.com/o/599648)‚Äù
- Ir at√© a aba Scrapy Cloud
- Criar um novo projeto clicando em ‚ÄúStart Project‚Äù 
- Em Scrapy Cloud, navegar no projeto criado
- Ir na sess√£o ‚ÄúSPIDERS/Code & Deploys‚Äù
- Clicar no bot√£o ‚ÄúDeploy My code‚Äù

Dando deploy do seu projeto

---
- No terminal do seu projeto instalar o ‚Äúshub‚Äù com o comando ‚Äúpip install shub‚Äù
- Terminal: ‚Äúshub login"
- Fornecer a chave de API ap√≥s ‚ÄúAPI key: SUA-CHAVE-API‚Äù
- Executar Deploy com o comando ‚Äúshub deploy PROJECT-ID‚Äù
- Aguardar execu√ß√£o do deploy e verificar no Dashboard

Rodando o projeto

---

- No menu de navega√ß√£o do site do Scrapy Cloud, ir em ‚ÄúJOBS/Dashboard‚Äù
- Clicar em ‚ÄúRun‚Äù
- Selecionar a Spider que deseja rodar
- Clicar em ‚ÄúRun‚Äù

*Nota: Caso n√£o seja importado automaticamente, adicionar ao arquivo ‚Äúscrapinghub.yml‚Äù a linha de c√≥digo ‚Äúrequirements_file: requirements.txt‚Äù para que seja mapeado o arquivo de requirements.txt do projeto.* [Ver tutorial](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
</details>

</aside>


<aside>
  
---
**Mais detalhes:**
<details id="subsecao-1">
<summary>Spider base criada a partir do comando ‚Äúscrapy genspyder bookspider book.toscrape.com‚Äù</summary>
   
   ```python

    import scrapy
    
    
    class BookspiderSpider(scrapy.Spider):
        name = "bookspider"
        allowed_domains = ["books.toscrape.com"]
        start_urls = ["https://books.toscrape.com"]
    
        def parse(self, response):
            pass
   ```

<p align="right">(<a href="#readme-top">Voltar ao topo</a>)</p>
</details>

<details id="subsecao-1-1">
<summary>1.1 Alterando o m√©todo parse para retornar um objeto com atributos vindos da response</summary>

  ```python
    import scrapy
    
    
    class BookspiderSpider(scrapy.Spider):
        name = "bookspider"
        allowed_domains = ["books.toscrape.com"]
        start_urls = ["https://books.toscrape.com"]
    
        def parse(self, response):
            pass
  ```

<p align="right">(<a href="#readme-top">Voltar ao topo</a>)</p>
</details>

<details id="subsecao-1-2">
<summary>1.2 Adicionando coleta de p√°ginas</summary>

  ```python
  import scrapy
  
  
  class BookspiderSpider(scrapy.Spider):
      name = "bookspider"
      allowed_domains = ["books.toscrape.com"]
      start_urls = ["https://books.toscrape.com"]
  
      def parse(self, response):
          books = response.css('article.product_pod')
  
          for book in books:
              yield{
                  'name': book.css('h3 a::text').get(),
                  'price': book.css('div p.price_color::text').get(),
                  'url': book.css('h3 a').attrib['href']
              }
  
  		#------------------------------------------------------------------------------------------------
          next_page = response.css('.next a::attr(href)').get()
  
          if next_page is not None:
              if 'catalogue/' in next_page:
                  next_page_url = 'https://books.toscrape.com/' + next_page
              else:
                  next_page_url = 'https://books.toscrape.com/catalogue/' + next_page
              yield response.follow(next_page_url, callback=self.parse)
  ```

<p align="right">(<a href="#readme-top">Voltar ao topo</a>)</p>
</details>

<details id="subsecao-2">
<summary>2.Adicionando raspagem de cada p√°gina do livro</summary>

  ```python
    import scrapy
    
    
    class BookspiderSpider(scrapy.Spider):
        name = "bookspider"
        allowed_domains = ["books.toscrape.com"]
        start_urls = ["https://books.toscrape.com"]
    
        def parse(self, response):
            books = response.css('article.product_pod')
    
    #---------------------------------------Alterado-------------------------------------------------
            for book in books:
                book_relative_url = book.css('h3 a::attr(href)').get()
                if 'catalogue/' in book_relative_url:
                    book_url = 'https://books.toscrape.com/' + book_relative_url
                else:
                    book_url = 'https://books.toscrape.com/catalogue/' + book_relative_url
                yield response.follow(book_url, callback=self.parse_book_page)
    #------------------------------------------------------------------------------------------------
    
            next_page = response.css('.next a::attr(href)').get()
            if next_page is not None:
                if 'catalogue/' in next_page:
                    next_page_url = 'https://books.toscrape.com/' + next_page
                else:
                    next_page_url = 'https://books.toscrape.com/catalogue/' + next_page
                yield response.follow(next_page_url, callback=self.parse)
    
    #----------------------------------------Adicionado----------------------------------------------
        def parse_book_page(self, response):
            table_rows = response.css("table tr")
            yield {
                'url': response.url,
                'title': response.css('.product_main h1::text').get(),
                'product_type': table_rows[1].css('td ::text').get(),
                'price_excl_tax': table_rows[2].css('td ::text').get(),
                'price_excl_tax': table_rows[2].css('td ::text').get(),
                'tax': table_rows[4].css('td ::text').get(),
                'availability': table_rows[5].css('td ::text').get(),
                'num_reviews': table_rows[6].css('td ::text').get(),
                'stars' : response.css('p.star-rating::attr(class)').get(),
                'category': response.xpath("//ul[@class='breadcrumb']/li[@class='active']/preceding-sibling::li[1]/a/text()").get(),
                'description': response.xpath("//div[@id='product_description']/following-sibling::p/text()").get(),
                'price': response.css('p.price_color::text').get()
            }
    
    """
    	Nota: Em category e description foi usado Xpath
    	no caso de category foi pego o elemento irm√£o (de mesmo nivel) anterior ao elemento li de classe 'active'
    	no caso da description foi pego o elemento irm√£o (de mesmo nivel) posterior ao elemento div de cladesse 'product_description'
    """
  ```

<p align="right">(<a href="#readme-top">Voltar ao topo</a>)</p>
</details>

<details id="subsecao-3">
<summary>3.Adicionando um item</summary>
  
```py
    #items.py
    
    class BookItem(scrapy.Item):
        url = scrapy.Field()
        title = scrapy.Field()
        upc = scrapy.Field()
        product_type = scrapy.Field()
        price_excl_tax = scrapy.Field()
        price_incl_tax = scrapy.Field()
        tax = scrapy.Field()
        availability = scrapy.Field()
        num_reviews = scrapy.Field()
        stars = scrapy.Field()
        category = scrapy.Field()
        description = scrapy.Field()
        price = scrapy.Field()
```
> Nota: ‚Äú√â poss√≠vel serializar um campo ou v√°rios campos criando uma fun√ß√£o e passando ela como parametro serializer.‚Äù

```py
    def serialize_price(value):
        return f'¬£ {str(value)}'
    
    
    class BookItem(scrapy.Item):
        url = scrapy.Field()
        title = scrapy.Field()
        upc = scrapy.Field()
        product_type = scrapy.Field()
        price_excl_tax = scrapy.Field()
        price_incl_tax = scrapy.Field()
        tax = scrapy.Field()
        availability = scrapy.Field()
        num_reviews = scrapy.Field()
        stars = scrapy.Field()
        category = scrapy.Field()
        description = scrapy.Field()
        price = scrapy.Field(serializer=serialize_price)
```

<p align="right">(<a href="#readme-top">Voltar ao topo</a>)</p>
</details>

<details id="subsecao-3-1">
<summary>3.1 Populando a classe</summary>

```py
    #bookspider.py
    
    import scrapy
    
    from bookscraper.items import BookItem
    
    
    class BookspiderSpider(scrapy.Spider):
        name = "bookspider"
        allowed_domains = ["books.toscrape.com"]
        start_urls = ["https://books.toscrape.com"]
    
        def parse(self, response):
            books = response.css('article.product_pod')
    
            for book in books:
                book_relative_url = book.css('h3 a::attr(href)').get()
                if 'catalogue/' in book_relative_url:
                    book_url = 'https://books.toscrape.com/' + book_relative_url
                else:
                    book_url = 'https://books.toscrape.com/catalogue/' + book_relative_url
                yield response.follow(book_url, callback=self.parse_book_page)
    
            next_page = response.css('.next a::attr(href)').get()
            if next_page is not None:
                if 'catalogue/' in next_page:
                    next_page_url = 'https://books.toscrape.com/' + next_page
                else:
                    next_page_url = 'https://books.toscrape.com/catalogue/' + next_page
                yield response.follow(next_page_url, callback=self.parse)
    
    #----------------------------Alterado---------------------------------------------
        def parse_book_page(self, response):
            table_rows = response.css("table tr")
            book_item = BookItem()
    
            book_item['url'] = response.url,
            book_item['title'] = response.css('.product_main h1::text').get(),
            book_item['upc'] = table_rows[0].css('td ::text').get(),
            book_item['product_type'] = table_rows[1].css('td ::text').get(),
            book_item['price_excl_tax'] = table_rows[2].css('td ::text').get(),
            book_item['price_incl_tax'] = table_rows[3].css('td ::text').get(),
            book_item['tax'] = table_rows[4].css('td ::text').get(),
            book_item['availability'] = table_rows[5].css('td ::text').get(),
            book_item['num_reviews'] = table_rows[6].css('td ::text').get(),
            book_item['stars'] = response.css('p.star-rating::attr(class)').get(),
            book_item['category'] = response.xpath("//ul[@class='breadcrumb']/li[@class='active']/preceding-sibling::li[1]/a/text()").get(),
            book_item['description'] = response.xpath("//div[@id='product_description']/following-sibling::p/text()").get(),
            book_item['price'] = response.css('p.price_color::text').get()
    
            yield book_item
```

<p align="right">(<a href="#readme-top">Voltar ao topo</a>)</p>
</details>

<details id="subsecao-4">
<summary>4 Criando uma pipeline</summary>

```py
    # Define your item pipelines here
    #
    # Don't forget to add your pipeline to the ITEM_PIPELINES setting
    # See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html
    
    
    # useful for handling different item types with a single interface
    from itemadapter import ItemAdapter
    
    
    class BookscraperPipeline:
    
        def process_item(self, item, spider):adapter = ItemAdapter(item)
    
            # Strip all whitespaces from strings
            field_names = adapter.field_names()
    
            for field_name in field_names:
                if field_name != 'description':
                    value = adapter.get(field_name)
                    if isinstance(adapter[field_name], tuple):
                        adapter[field_name] = value[0].strip()
                    else:
                        adapter[field_name] = value.strip()
                else:
                    value = adapter.get(field_name)
                    adapter[field_name] = str(value[0])
    
            # Category & Product Type --> switch to lowercase
            lowercase_keys = ['category', 'product_type']
            for lowercase_key in lowercase_keys:
                value = adapter.get(lowercase_key)
                adapter[lowercase_key] = value.lower()
    
            # Price --> convert to float
            price_keys = ['price_excl_tax', 'price_incl_tax', 'tax', 'price']
            for price_key in price_keys:
                value = adapter.get(price_key)
                value = value.replace('¬£', '')
                adapter[price_key] = float(value)
    
            # Availability --> extract number of books in stock (removing strings)
            availability_string = adapter.get('availability')
            split_string_array = availability_string.split('(')
            if len(split_string_array) < 2:
                adapter['availability'] = 0
            else:
                availability_string = split_string_array[1].split(' ')
                adapter['availability'] = int(availability_string[0])
    
            # Reviews --> convert string to int
            num_reviews_string = adapter.get('num_reviews')
            adapter['num_reviews'] = int(num_reviews_string)
    
            # Stars --> covert text to number
            stars_string = adapter.get('stars')
            split_stars_array = stars_string.split(' ')
            stars_text_value = split_stars_array[1].lower()
            if stars_text_value == "zero":
                adapter['stars'] = 0
            elif stars_text_value == "one":
                adapter['stars'] = 1
            elif stars_text_value == "two":
                adapter['stars'] = 2
            elif stars_text_value == "three":
                adapter['stars'] = 3
            elif stars_text_value == "four":
                adapter['stars'] = 4
            elif stars_text_value == "five":
                adapter['stars'] = 5
    
            return item
```

<p align="right">(<a href="#readme-top">Voltar ao topo</a>)</p>
</details>

<details id="subsecao-5">
<summary>5 Classe para lidar com o banco</summary>
    
> Nota: Os metodos open_spider,  close_spider  e  process_item ser√£o encontrados pelo scrapy e s√£o executados automaticamente no devido tempo de vida da spider.

```py
    #pipelines.py
    
    class SaveToSQLitePipeline:
    
        def open_spider(self, spider):
            self.conn = sqlite3.connect('books.db')
            self.cur = self.conn.cursor()
            self.cur.execute("""
            CREATE TABLE IF NOT EXISTS books (
                id INTEGER PRIMARY KEY,
                url TEXT,
                title TEXT,
                upc TEXT,
                product_type TEXT,
                price_excl_tax REAL,
                price_incl_tax REAL,
                tax REAL,
                availability INTEGER,
                num_reviews INTEGER,
                stars REAL,
                category TEXT,
                description TEXT,
                price REAL
            );
            """)
    
        def close_spider(self, spider):
            self.conn.close()
    
        def process_item(self, item, spider):
            query = """
                INSERT INTO books (
                    url, title, upc, product_type, price_excl_tax, price_incl_tax,
                    tax, availability, num_reviews, stars, category, description, price
                )
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """
    
            values = (
                item['url'],
                item['title'],
                item['upc'],
                item['product_type'],
                item['price_excl_tax'],
                item['price_incl_tax'],
                item['tax'],
                item['availability'],
                item['num_reviews'],
                item['stars'],
                item['category'],
                item['description'],
                item['price']
            )
    
            self.cur.execute(query, values)
            self.conn.commit()
            return item
```

<p align="right">(<a href="#readme-top">Voltar ao topo</a>)</p>
</details>

<details id="subsecao-6">
<summary>6 Criando cabe√ßalhos (headers)</summary>

```py
    #middlewares.py
    
    class ScrapeOpsFakeBrowserHeaderAgentMiddleware:
    
        @classmethod
        def from_crawler(cls, crawler):
            return cls(crawler.settings)
    
        def __init__(self, settings):
            self.scrapeops_api_key = settings.get('SCRAPEOPS_API_KEY')
            self.scrapeops_endpoint = settings.get('SCRAPEOPS_FAKE_BROWSER_HEADER_ENDPOINT')
            self.scrapeops_fake_browser_headers_active = settings.get('SCRAPEOPS_FAKE_BROWSER_HEADER_ENABLED')
            self.scrapeops_num_results = settings.get('SCRAPEOPS_NUM_RESULTS')
            self.headers_list = []
            self._get_headers_list()
            self._scrapeops_fake_browser_headers_enabled()
    
        def _get_headers_list(self):
            payload = {'api_key': self.scrapeops_api_key}
            if self.scrapeops_num_results is not None:
                payload['num_results'] = self.scrapeops_num_results
            response = requests.get(self.scrapeops_endpoint, params=urlencode(payload))
            json_response = response.json()
            self.headers_list = json_response.get('result', [])
    
        def _get_random_browser_header(self):
            random_index = randint(0, len(self.headers_list) - 1)
            return self.headers_list[random_index]
    
        def _scrapeops_fake_browser_headers_enabled(self):
            if self.scrapeops_api_key is None or self.scrapeops_api_key == '' or self.scrapeops_fake_browser_headers_active == False:
                self.scrapeops_fake_browser_headers_active = False
            else:
                self.scrapeops_fake_browser_headers_active = True
    
        def process_request(self, request, spider):
            random_browser_header = self._get_random_browser_header()
            request.headers['user-agent'] = random_browser_header['user-agent']
            request.headers['accept'] = random_browser_header['accept']
            request.headers['sec-ch-ua'] = random_browser_header['sec-ch-ua']
            request.headers['sec-ch-ua-mobile'] = random_browser_header['sec-ch-ua-mobile']
            request.headers['sec-ch-ua-platform'] = random_browser_header['sec-ch-ua-platform']
            request.headers['sec-fetch-site'] = random_browser_header['sec-fetch-site']
            request.headers['sec-fetch-mod'] = random_browser_header['sec-fetch-mod']
            request.headers['sec-fetch-user'] = random_browser_header['sec-fetch-user']
            request.headers['accept-encoding'] = random_browser_header['accept-encoding']
            request.headers['accept-language'] = random_browser_header['accept-language']
    
            print(random_browser_header)
```

<p align="right">(<a href="#readme-top">Voltar ao topo</a>)</p>
</details>

<details id="subsecao-6-1">
<summary>6.1 Adicionando vari√°veis</summary>

```py
    #settings.py
    
    SCRAPEOPS_API_KEY = 'a889d691-4e9a-4c25-a26b-a7faec67ebfe' #api-key-just-for-exampl;e
    
    # SCRAPEOPS_FAKE_USER_AGENT_ENDPOINT = 'https://headers.scrapeops.io/v1/user-agents'
    SCRAPEOPS_FAKE_BROWSER_HEADER_ENDPOINT = 'https://headers.scrapeops.io/v1/browser-headers'
    
    # SCRAPEOPS_FAKE_USER_AGENT_ENABLED = True
    SCRAPEOPS_FAKE_BROWSER_HEADER_ENABLED = True
    
    SCRAPEOPS_NUM_RESULTS = 50
```

<p align="right">(<a href="#readme-top">Voltar ao topo</a>)</p>
</details>

<details id="subsecao-6-2">
<summary>6.2 Adicionar a classe ao middlewares</summary>

```py
    #settings.py
    
    DOWNLOADER_MIDDLEWARES = {
       # "bookscraper_project.middlewares.BookscraperDownloaderMiddleware": 543,
       # 'bookscraper.middlewares.ScrapeOpsFakeUserAgentMiddleware': 400,
       'bookscraper.middlewares.ScrapeOpsFakeBrowserHeaderAgentMiddleware': 400,
    }
```

<p align="right">(<a href="#readme-top">Voltar ao topo</a>)</p>
</details>

<details id="subsecao-7">
<summary>7 Configura√ß√µes de proxy</summary>

> Terminal:  ‚Äúpip install scrapy-rotating-proxies‚Äù

Adicionar vari√°veis de ambiente


```py
    #settings.py
    
    ROTATING_PROXY_LIST = [
       '45.70.204.233:4145',
       '45.7.177.224:39867',
       '187.73.55.66:5678',
    ]
    
    DOWNLOADER_MIDDLEWARES = {
       # "bookscraper_project.middlewares.BookscraperDownloaderMiddleware": 543,
       # 'bookscraper.middlewares.ScrapeOpsFakeUserAgentMiddleware': 400,
       'bookscraper.middlewares.ScrapeOpsFakeBrowserHeaderAgentMiddleware': 400,
       'rotating_proxies.middlewares.RotatingProxyMiddleware': 610,
       'rotating_proxies.middlewares.BanDetectionMiddleware': 620,
    }
```
<p align="right">(<a href="#readme-top">Voltar ao topo</a>)</p>
</details>

<details id="subsecao-7-1">
<summary>7.1 Alternativa usando servi√ßo pago smart proxy</summary>

Site: [https://dashboard.smartproxy.com](https://dashboard.smartproxy.com/login)

Tutorial: [freeCode](https://thepythonscrapyplaybook.com/freecodecamp-beginner-course/freecodecamp-scrapy-beginners-course-part-9-rotating-proxies/)

Depois de comprar um plano seguir os passos:

> Criar um usuario

![create_user](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/7aea2b02-01da-4378-8dca-4f17c00a5206)

> Adicionar configura√ß√µes

![add_configs](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/5bbdbdb5-3be3-4ecd-b7f0-b0262af0069e)

> Obter o endpoint

![get_endpoints](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/01ed0dec-f6f7-49a6-ae0e-5c497610483b)

> Criar uma classe para o proxy

![create_class_proxy](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/76f1f1e3-e789-4a7f-9bbd-0e5700ae716b)
![create_class_proxy_2](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/ca277335-814b-4585-a791-2aefacb1f64d)

> Adicionar vari√°veis de ambiente

![add_env_vars](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/4c808bc8-35d5-4023-bdfc-b3ef30ca8698)

>Adicionar middleware

![add_middleware](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/814f772e-e789-40d0-bf92-bcdde148ec4c)

<p align="right">(<a href="#readme-top">Voltar ao topo</a>)</p>
</details>

<details id="subsecao-7-2">
<summary>7.2 Usando scrapeops proxy api</summary>

> Terminal: ‚Äúpip install scrapeops-scrapy-proxy-sdk‚Äù

Adicionar vari√°vel de ambiente
```py
    # settings.py
    
    SCRAPEOPS_API_KEY = 'YOUR_API_KEY'
    SCRAPEOPS_PROXY_ENABLED = True
    SCRAPEOPS_PROXY_SETTINGS = {'country': 'us'}
    
    DOWNLOADER_MIDDLEWARES = {
        'scrapeops_scrapy_proxy_sdk.scrapeops_scrapy_proxy_sdk.ScrapeOpsScrapyProxySdk': 725,
    }
```

Adicionar middleware para gerenciar as urls.

```py
    ## middlewares.py
    
    from urllib.parse import urlencode
    from scrapy import Request
    
    class ScrapeOpsProxyMiddleware:
    
        @classmethod
        def from_crawler(cls, crawler):
            return cls(crawler.settings)
    
        def __init__(self, settings):
            self.scrapeops_api_key = settings.get('SCRAPEOPS_API_KEY')
            self.scrapeops_endpoint = 'https://proxy.scrapeops.io/v1/?'
            self.scrapeops_proxy_active = settings.get('SCRAPEOPS_PROXY_ENABLED', False)
    				self.scrapeops_proxy_settings = {}
            self._clean_proxy_settings(settings.get('SCRAPEOPS_PROXY_SETTINGS'))
    
        @staticmethod
        def _replace_response_url(response):
            real_url = response.headers.get(
                'Sops-Final-Url', def_val=response.url)
            return response.replace(
                url=real_url.decode(response.headers.encoding))
        
        def _clean_proxy_settings(self, proxy_settings):
            if proxy_settings is not None:
                for key, value in proxy_settings.items():
                    clean_key = key.replace('sops_', '')
                    self.scrapeops_proxy_settings[clean_key] = value
        
        def _get_scrapeops_url(self, request):
            payload = {'api_key': self.scrapeops_api_key, 'url': request.url}
            
            ## Global Request Settings
            if self.scrapeops_proxy_settings is not None:
                for key, value in self.scrapeops_proxy_settings.items():
                    payload[key] = value
    
            ## Request Level Settings 
            for key, value in request.meta.items():
                if 'sops_' in key:
                    clean_key = key.replace('sops_', '')
                    payload[clean_key] = value
    
            proxy_url = self.scrapeops_endpoint + urlencode(payload)
            return proxy_url
    
        def _scrapeops_proxy_enabled(self):
            if self.scrapeops_api_key is None or self.scrapeops_api_key == '' or self.scrapeops_proxy_active == False:
                return False
            return True
        
        def process_request(self, request, spider):
            if self._scrapeops_proxy_enabled is False or self.scrapeops_endpoint in request.url:
                return None
            
            scrapeops_url = self._get_scrapeops_url(request)
            new_request = request.replace(
                cls=Request, url=scrapeops_url, meta=request.meta)
            return new_request
    
        def process_response(self, request, response, spider):
            new_response = self._replace_response_url(response)
            return new_response
```

Resultado:
![result](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/e8121f43-6645-4184-a77f-5e60505cbc78)

<p align="right">(<a href="#readme-top">Voltar ao topo</a>)</p>   
</details>

<details id="subsecao-8">
<summary>8 Rodando projeto na nuvem usando Scrapy Cloud</summary>

>Criar uma conta em ‚Äúhttps://app.zyte.com/account/login/?next=/o/599648/scrapycloud/discover‚Äù

![1](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/f14d3352-e74f-44ec-8a8b-2214677a287b)

>no menu lateral navegar at√© scrapy Cloud

![2](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/9478d467-0763-4379-a149-27742b2f1762)

>Clicar em start project

![3](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/d34eaf9a-71b9-4f7c-8dcb-0b9283ccfe11)

>Dar um nome ao projeto

![6](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/c382ebc5-e95b-4fda-8f77-b7146a582a1a)

>Navegar a pagina de Code & Deploy 1.0

![7](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/0f8e21b3-3d58-48de-9c40-4d57b185a9cd)

>Navegar a pagina de Code & Deploy 1.1

![8](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/fb7c767f-9f90-4c3d-91aa-32db42826555)

>Instalar shub no projeto (O projeto atual foi criado no Pycharm  ent√£o foi usado o pr√≥prio console do Pycharm)

![9](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/25680d88-d3b6-4605-89b1-5cd735aee312)

>Logar no shub 1.0: digitar comando de login

![10](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/c84f7e7c-e005-430d-b8e6-43beed645f2e)

>Logar no shub 1.1: Inserir chave API 

![11](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/ba94fcc6-bcc0-4228-824d-de03c03f51e4)

![11 1](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/5b144044-988d-488f-8ce7-d7d2bca436de)
>Executar Deploy do projeto para o shub

![12](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/34e4c303-5417-45ef-9d11-033bfeb55b56)

>Aguardar o Deplyoy

![13](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/5dc44f12-d4c8-4267-a09f-0977d543cb41)

>Sucesso!

![14](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/cbd4e79b-aaaf-4d1b-8aac-b0fac2ddbf16)

>Inserir o nome da Spider e Clicar em Run

![15](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/098243d2-58c8-4323-86ca-f819ff853570)

>Navegar at√© o Dashboard do projeto e clicar em Run

![16](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/16a897ba-3124-4e45-a9db-d046f3f7ee99)

>Virificar conclus√£o na se√ß√£o ‚ÄúCompleted‚Äù

![17](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/5eb27a5c-c670-4daa-8396-48a228990970)

>Acompanhar execu√ß√£o na se√ß√£o ‚ÄúRunning‚Äù

![18](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/0433c7a6-77cb-4240-9b99-4e938e36344c)

>No caso atual ocorreram 4 erros, para investigar melhor clicaremos no numero logo abaixo da coluna ‚ÄúErrors‚Äùna tabela da se√ß√£o ‚ÄúCompleted‚Äù.

![19](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/bdda0a79-8ee7-4395-a936-ebab225b7fed)

>Podemos investigar os erros na seguinte p√°gina, podendo expandir cada erro para ver o log do erro em espec√≠fico. 

![20](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/27b63610-61b0-479f-813d-34a5b4fd25ca)

>O erro √© decorrido da falta do m√≥dulo ‚Äúitemadapter‚Äù que utilizamos em uma classe dentro do arquivo pupelines.py para indicar um livro como um item

![21](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/86bd34fb-16b4-4db0-9bcd-7379381b4e35)

>O erro em quest√£o est√° ocorrendo pois no deploy do projeto n√£o estamos indicando os requisitos que o projeto est√° utilizando. 
para isso criaremos um arquivo requirements.txt na raiz do nosso projeto e adicionaremos a vers√£o do itemadapter que estamos utilizando

![22](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/483d0c3a-37dd-46c2-a473-2dc2910eeb95)

>Em seguida no arquivo scrapinghub.yml adicionaremos a linha de c√≥digo ‚Äúrequirements_file: requirements.txt‚Äù logo a baixo do id 
do projeto para indicar o caminho do nosso arquivo requirements.

![23](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/24e31841-4d24-40f3-9c15-3bcad6007d66)

>Executaremos novamente o Deploy do projeto

![24](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/07665c05-e8c2-4590-a629-582404c3c0e5)

>No site de gerenciamento de deploy iremos navegar at√© a se√ß√£o de deploy para verificar o andamento do nosso deploy

![25](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/1b72b23c-7a50-452d-bb9e-cc4593dd2180)

>Aguardaremos o progesso do deploy. Note que temos um numero 2 que indica que esse √© o segundo deploy desse projeto.

![26](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/b0070adf-edfc-4adb-a19e-f72bd6276be5)

>Indica√ß√£o de sucesso do deploy.

![27](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/cc63eeeb-722e-4211-a373-dcde04cb6197)

>Podemos clicar na se√ß√£o que acabamos de  implantar(Deploy) e navegar at√© a aba ‚ÄúRequirements‚Äù para 
verificar que nosso requirements.txt foi implementado com sucesso! 

![28](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/1ed4dd57-b921-4e67-95aa-63293c520394)

>Seguiremos os passos de execu√ß√£o da spider e se tudo correr bem teremos uma execu√ß√£o em andamento da
nossa spyder. caso contrario investigue os erros para resolver o problema.

![29](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/a3464210-1c88-4e56-b367-22e4a941e4f7)

>Agora no Dashboard podemos ver que a spyder rodou com sucesso e tambem podemos ver a que falhou.

![30](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/55232194-f55c-4ae6-b8e1-e9e0b3a67739)

>Clicando no numero do item podemos verificar os itens coletados.

![31](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/3d15d95c-5dcc-491f-a8e4-75fc2fcd6a7d)

>Nessa parte tamb√©m √© poss√≠vel baixar todos os itens em um formato desejado dentre os disponiveis.

![32](https://github.com/Lucas382/ScrapyTemplate/assets/44009909/7f2d948a-2dea-4f75-801d-2071c0388231)


<p align="right">(<a href="#readme-top">Voltar ao topo</a>)</p> 
</details>
<aside>
