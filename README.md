<aside>
üë£ *Passo a passo:*

---
**Instala√ß√£o e primeiro contato:**

<details>
<summary>Instala√ß√£o Scrapy</summary>

  -  Criar um novo projeto com ambiente virtual
  -  Abrir terminal e instalar o Scrapy com o comando ‚Äúpip install Scrapy‚Äù
</details>

<details>
<summary>Criar um projeto de Spider</summary>

-  Terminal: ‚Äúscrapy startproject nomedoprojetoscraper‚Äù
</details>

<details>
<summary>Criar uma Spider</summary>

-  Navegar at√© a pasta ‚Äúspiders‚Äù do projeto Scrapy criado
-  Terminal: ‚Äúscrapy genspider spidername site.to.scrape.com‚Äù
</details>

<details>
<summary>Instalar ipython para facilitar os testes</summary>

-  Terminal: ‚Äúpip install ipython‚Äù
-  No arquivo scrapy.cfg adicionar ‚ÄúSHELL= ipython‚Äù em baixo de default
</details>

<details>
<summary>Usando o terminal Ipython</summary>

-  Terminal: ‚Äúscrapy shell‚Äù para ativar o terminal ipython
-  No terminal Ipython (chamaremos de ITerminal):  ‚Äúfetch('https://books.toscrape.com/')‚Äù
-  ITerminal: ‚Äúresponse‚Äù ‚Üê <200 https://books.toscrape.com/>
-  Selecionando um elemento Iterminal: ‚Äúresponse.css('article.product_pod')‚Äù
-  Sair do ITerminal usando: ‚Äúexit‚Äù

- Tipos de sele√ß√£o Ipython:
  -  Salvar em uma vari√°vel: ‚Äúbooks = response.css('article.product_pod')‚Äù
  -  Verificar tamanho da lista da vari√°vel: ‚Äúlen(books)‚Äù
  -  Obter um item da lista: ‚Äúbook = books[0]‚Äù
  -  Obter o texto de um elemento ‚Äúa‚Äù dentro de um elemento ‚Äúh3‚Äù: ‚Äúbook.css('h3 a::text').get()‚Äù
  -  Obter o texto de um elemento dentro de outro elemento pela Classe: ‚Äúbook.css('.product_price .price_color::text').get()‚Äù
  -  Obter o conte√∫do de um atributo de um elemento: ‚Äúbook.css('h3 a').attrib['href']‚Äù ou ‚Äúresponse.css('.next a::attr(href)').get()‚Äù
</details>

<details>
<summary>Alterando uma Spider [nomedaspider.py](http://nomedaspider.py) (‚Äùbookspider.py‚Äù)</summary>

-  Alterar o m√©todo parse que receber√° a response igual ao ITerminal [1.1](#subsecao-1-1)
</details>

<details>
<summary>Ativando e testando uma Spider pelo terminal</summary>

-  Navegar at√© a pasta do projeto Scrapy, nesse caso bookscraper
-  Terminal: ‚Äúscrapy crawl bookspider‚Äù  Nota: o item ‚Äú'item_scraped_count': 20‚Äù referencia a quantidade de itens raspados.
</details>

<details>
<summary>Percorrendo pr√≥ximas p√°ginas</summary>

-  Encontrar o elemento respons√°vel pelo link da pr√≥xima p√°gina, nesse caso, ‚Äúresponse.css('.next a::attr(href)').get()‚Äù
-  Adicionar l√≥gica para percorrer pr√≥ximas p√°ginas usando callback ‚Äúyield response.follow(next_page_url, callback=self.parse)‚Äù [1.2](#subsecao-1-2)
</details>
</aside>

<aside>
  
---
**Intermedi√°rio:**

<details>
<summary>Adicionando Coleta de p√°ginas dentro da p√°gina principal</summary>

- Adicionar coleta de cada url de uma p√°gina do livro
- Adicionar m√©todo para coletar informa√ß√µes de uma p√°gina usando callback ‚Äúyield response.follow(book_url, callback=self.parse_book_page)‚Äù [2](#subsecao-2)
</details>

<details>
<summary>Exportar os dados manualmente para um arquivo</summary>

- Terminal: ‚Äúscrapy crawl bookspider -O bookdata.csv‚Äù para criar em formato csv
- Terminal: ‚Äúscrapy crawl bookspider -O bookdata.json‚Äù para criar em formato json
- Terminal: ‚Äúscrapy crawl bookspider -o bookdata.json‚Äù para adicionar em formato json
</details>

<details>
<summary>Criando e populando um scrapy Item</summary>

- Criar uma classe para o item no arquivo items.py [3.](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
- Popular a classe do livro [3.1](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
</details>

<details>
<summary>Rodando uma pipeline para pr√©-processar um item</summary>

- Habilitar a spider no arquivo de configura√ß√µes da spider ‚Äúsettings.py‚Äù descomentando o objeto ITEM_PIPELINES = {}
  Nota: Em ‚ÄúITEM_PIPELINES = {"bookscraper.pipelines.BookscraperPipeline": 300, }‚Äù o n√∫mero 300 representa a prioridade, onde, quanto menor o valor mais ‚Äúcedo‚Äù a pipeline vai rodar.
- Alterar o m√©todo process_item do item no arquivo ‚Äúpipelines.py‚Äù [4.](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
</details>

<details>
<summary>Salvando em um banco de dados</summary>

- Podemos adicionar no arquivo de configura√ß√µes da spider ‚Äúsettings.py‚Äù um objeto FEEDS = {'booksdata.json': {'format': 'json'}} para definir um formato padr√£o de sa√≠da quando rodarmos o comando ‚Äúscrapy crawl bookspider‚Äù que √© equivalente ao comando ‚Äúscrapy crawl bookspider -O cleandata.json‚Äù
- Podemos tamb√©m sobrescrever uma configura√ß√£o do arquivo settings.py dentro do arquivo bookspider.py (arquivo referente √† spider), basta adicionar  ‚Äúcustom_settings = { 'FEEDS': { 'booksdata.json': {'format': 'json'}, 'overwrite': True }}‚Äù ao c√≥digo
- Baixar e instalar SQLITE.
- Adicionar uma classe no arquivo pipelines.py para lidar com o banco [5.](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
- Adicionar um item no objeto ITEM_PIPELINES no arquivo settings.py com: "bookscraper.pipelines.SaveToSQLitePipeline": 400,"
</details>

<details>
<summary>Evitando bloqueio de requisi√ß√£o de um site</summary>

- Criar m√©todo para adicionar header ao request [6.](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
- Adicionar vari√°veis que o m√©todo necessitar [6.1](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
- Adicionar middleware ao arquivo settings.py [6.2](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
</details>

<details>
<summary>Adicionando proxy √†s requisi√ß√µes</summary>

- Adicionar configura√ß√£o de proxy ao arquivo settings.py [7.](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
- Alternativa usando um servi√ßo pago [7.1](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
- Alternativa usando scrapeops proxy api [7.2](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
</details>

<details>
<summary>Rodando uma Spider na nuvem </summary>[8.](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)

Criando um projeto Scrapy Cloud

---
- Criar uma conta em ‚Äú[https://app.zyte.com](https://app.zyte.com/o/599648)‚Äù
- Ir at√© a aba Scrapy Cloud
- Criar um novo projeto clicando em ‚ÄúStart Project‚Äù
- Em Scrapy Cloud, navegar no projeto criado
- Ir na sess√£o ‚ÄúSPIDERS/Code & Deploys‚Äù
- Clicar no bot√£o ‚ÄúDeploy My code‚Äù

Dando deploy do seu projeto

---
- No terminal do seu projeto instalar o ‚Äúshub‚Äù com o comando ‚Äúpip install shub‚Äù
- Terminal: ‚Äúshub login"
- Fornecer a chave de API ap√≥s ‚ÄúAPI key: SUA-CHAVE-API‚Äù
- Executar Deploy com o comando ‚Äúshub deploy PROJECT-ID‚Äù
- Aguardar execu√ß√£o do deploy e verificar no Dashboard

Rodando o projeto

---

- No menu de navega√ß√£o do site do Scrapy Cloud, ir em ‚ÄúJOBS/Dashboard‚Äù
- Clicar em ‚ÄúRun‚Äù
- Selecionar a Spider que deseja rodar
- Clicar em ‚ÄúRun‚Äù

*Nota: Caso n√£o seja importado automaticamente, adicionar ao arquivo ‚Äúscrapinghub.yml‚Äù a linha de c√≥digo ‚Äúrequirements_file: requirements.txt‚Äù para que seja mapeado o arquivo de requirements.txt do projeto.* [Ver tutorial](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
</details>
</aside>


<aside>
  
---
**Mais detalhes:**
<details>
<summary>Spider base criada a partir do comando ‚Äúscrapy genspyder bookspider book.toscrape.com‚Äù</summary>
   
   ```python

    import scrapy
    
    
    class BookspiderSpider(scrapy.Spider):
        name = "bookspider"
        allowed_domains = ["books.toscrape.com"]
        start_urls = ["https://books.toscrape.com"]
    
        def parse(self, response):
            pass
   ```
</details>

<details id="subsecao-1-1">
<summary>1.1 Alterando o m√©todo parse para retornar um objeto com atributos vindos da response</summary>

  ```python
    import scrapy
    
    
    class BookspiderSpider(scrapy.Spider):
        name = "bookspider"
        allowed_domains = ["books.toscrape.com"]
        start_urls = ["https://books.toscrape.com"]
    
        def parse(self, response):
            pass
  ```
  
</details>

<details id="subsecao-1-2">
<summary>1.2 Adicionando coleta de p√°ginas</summary>

  ```python
  import scrapy
  
  
  class BookspiderSpider(scrapy.Spider):
      name = "bookspider"
      allowed_domains = ["books.toscrape.com"]
      start_urls = ["https://books.toscrape.com"]
  
      def parse(self, response):
          books = response.css('article.product_pod')
  
          for book in books:
              yield{
                  'name': book.css('h3 a::text').get(),
                  'price': book.css('div p.price_color::text').get(),
                  'url': book.css('h3 a').attrib['href']
              }
  
  		#------------------------------------------------------------------------------------------------
          next_page = response.css('.next a::attr(href)').get()
  
          if next_page is not None:
              if 'catalogue/' in next_page:
                  next_page_url = 'https://books.toscrape.com/' + next_page
              else:
                  next_page_url = 'https://books.toscrape.com/catalogue/' + next_page
              yield response.follow(next_page_url, callback=self.parse)
  ```
  
</details>

<details id="subsecao-2">
<summary>2.Adicionando raspagem de cada p√°gina do livro</summary>

  ```python
    import scrapy
    
    
    class BookspiderSpider(scrapy.Spider):
        name = "bookspider"
        allowed_domains = ["books.toscrape.com"]
        start_urls = ["https://books.toscrape.com"]
    
        def parse(self, response):
            books = response.css('article.product_pod')
    
    #---------------------------------------Alterado-------------------------------------------------
            for book in books:
                book_relative_url = book.css('h3 a::attr(href)').get()
                if 'catalogue/' in book_relative_url:
                    book_url = 'https://books.toscrape.com/' + book_relative_url
                else:
                    book_url = 'https://books.toscrape.com/catalogue/' + book_relative_url
                yield response.follow(book_url, callback=self.parse_book_page)
    #------------------------------------------------------------------------------------------------
    
            next_page = response.css('.next a::attr(href)').get()
            if next_page is not None:
                if 'catalogue/' in next_page:
                    next_page_url = 'https://books.toscrape.com/' + next_page
                else:
                    next_page_url = 'https://books.toscrape.com/catalogue/' + next_page
                yield response.follow(next_page_url, callback=self.parse)
    
    #----------------------------------------Adicionado----------------------------------------------
        def parse_book_page(self, response):
            table_rows = response.css("table tr")
            yield {
                'url': response.url,
                'title': response.css('.product_main h1::text').get(),
                'product_type': table_rows[1].css('td ::text').get(),
                'price_excl_tax': table_rows[2].css('td ::text').get(),
                'price_excl_tax': table_rows[2].css('td ::text').get(),
                'tax': table_rows[4].css('td ::text').get(),
                'availability': table_rows[5].css('td ::text').get(),
                'num_reviews': table_rows[6].css('td ::text').get(),
                'stars' : response.css('p.star-rating::attr(class)').get(),
                'category': response.xpath("//ul[@class='breadcrumb']/li[@class='active']/preceding-sibling::li[1]/a/text()").get(),
                'description': response.xpath("//div[@id='product_description']/following-sibling::p/text()").get(),
                'price': response.css('p.price_color::text').get()
            }
    
    """
    	Nota: Em category e description foi usado Xpath
    	no caso de category foi pego o elemento irm√£o (de mesmo nivel) anterior ao elemento li de classe 'active'
    	no caso da description foi pego o elemento irm√£o (de mesmo nivel) posterior ao elemento div de cladesse 'product_description'
    """
  ```
  
</details>
<aside>
