<aside>
ğŸ‘£ *Passo a passo:*

---
**InstalaÃ§Ã£o e primeiro contato:**

<details>
<summary>InstalaÃ§Ã£o Scrapy</summary>

  -  Criar um novo projeto com ambiente virtual
  -  Abrir terminal e instalar o Scrapy com o comando â€œpip install Scrapyâ€
</details>

<details>
<summary>Criar um projeto de Spider</summary>

-  Terminal: â€œscrapy startproject nomedoprojetoscraperâ€
</details>

<details>
<summary>Criar uma Spider</summary>

-  Navegar atÃ© a pasta â€œspidersâ€ do projeto Scrapy criado
-  Terminal: â€œscrapy genspider spidername site.to.scrape.comâ€
</details>

<details>
<summary>Instalar ipython para facilitar os testes</summary>

-  Terminal: â€œpip install ipythonâ€
-  No arquivo scrapy.cfg adicionar â€œSHELL= ipythonâ€ em baixo de default
</details>

<details>
<summary>Usando o terminal Ipython</summary>

-  Terminal: â€œscrapy shellâ€ para ativar o terminal ipython
-  No terminal Ipython (chamaremos de ITerminal):  â€œfetch('https://books.toscrape.com/')â€
-  ITerminal: â€œresponseâ€ â† <200 https://books.toscrape.com/>
-  Selecionando um elemento Iterminal: â€œresponse.css('article.product_pod')â€
-  Sair do ITerminal usando: â€œexitâ€

- Tipos de seleÃ§Ã£o Ipython:
  -  Salvar em uma variÃ¡vel: â€œbooks = response.css('article.product_pod')â€
  -  Verificar tamanho da lista da variÃ¡vel: â€œlen(books)â€
  -  Obter um item da lista: â€œbook = books[0]â€
  -  Obter o texto de um elemento â€œaâ€ dentro de um elemento â€œh3â€: â€œbook.css('h3 a::text').get()â€
  -  Obter o texto de um elemento dentro de outro elemento pela Classe: â€œbook.css('.product_price .price_color::text').get()â€
  -  Obter o conteÃºdo de um atributo de um elemento: â€œbook.css('h3 a').attrib['href']â€ ou â€œresponse.css('.next a::attr(href)').get()â€
</details>

<details>
<summary>Alterando uma Spider [nomedaspider.py](http://nomedaspider.py) (â€bookspider.pyâ€)</summary>

-  Alterar o mÃ©todo parse que receberÃ¡ a response igual ao ITerminal [SeÃ§Ã£o 1](#SeÃ§Ã£o-1)
</details>

<details>
<summary>Ativando e testando uma Spider pelo terminal</summary>

-  Navegar atÃ© a pasta do projeto Scrapy, nesse caso bookscraper
-  Terminal: â€œscrapy crawl bookspiderâ€  Nota: o item â€œ'item_scraped_count': 20â€ referencia a quantidade de itens raspados.
</details>

<details>
<summary>Percorrendo prÃ³ximas pÃ¡ginas</summary>

-  Encontrar o elemento responsÃ¡vel pelo link da prÃ³xima pÃ¡gina, nesse caso, â€œresponse.css('.next a::attr(href)').get()â€
-  Adicionar lÃ³gica para percorrer prÃ³ximas pÃ¡ginas usando callback â€œyield response.follow(next_page_url, callback=self.parse)â€ [1.2](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
</details>
</aside>

<aside>
  
---
**IntermediÃ¡rio:**

<details>
<summary>Adicionando Coleta de pÃ¡ginas dentro da pÃ¡gina principal</summary>

- Adicionar coleta de cada url de uma pÃ¡gina do livro
- Adicionar mÃ©todo para coletar informaÃ§Ãµes de uma pÃ¡gina usando callback â€œyield response.follow(book_url, callback=self.parse_book_page)â€ [2.](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
</details>

<details>
<summary>Exportar os dados manualmente para um arquivo</summary>

- Terminal: â€œscrapy crawl bookspider -O bookdata.csvâ€ para criar em formato csv
- Terminal: â€œscrapy crawl bookspider -O bookdata.jsonâ€ para criar em formato json
- Terminal: â€œscrapy crawl bookspider -o bookdata.jsonâ€ para adicionar em formato json
</details>

<details>
<summary>Criando e populando um scrapy Item</summary>

- Criar uma classe para o item no arquivo items.py [3.](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
- Popular a classe do livro [3.1](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
</details>

<details>
<summary>Rodando uma pipeline para prÃ©-processar um item</summary>

- Habilitar a spider no arquivo de configuraÃ§Ãµes da spider â€œsettings.pyâ€ descomentando o objeto ITEM_PIPELINES = {}
  Nota: Em â€œITEM_PIPELINES = {"bookscraper.pipelines.BookscraperPipeline": 300, }â€ o nÃºmero 300 representa a prioridade, onde, quanto menor o valor mais â€œcedoâ€ a pipeline vai rodar.
- Alterar o mÃ©todo process_item do item no arquivo â€œpipelines.pyâ€ [4.](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
</details>

<details>
<summary>Salvando em um banco de dados</summary>

- Podemos adicionar no arquivo de configuraÃ§Ãµes da spider â€œsettings.pyâ€ um objeto FEEDS = {'booksdata.json': {'format': 'json'}} para definir um formato padrÃ£o de saÃ­da quando rodarmos o comando â€œscrapy crawl bookspiderâ€ que Ã© equivalente ao comando â€œscrapy crawl bookspider -O cleandata.jsonâ€
- Podemos tambÃ©m sobrescrever uma configuraÃ§Ã£o do arquivo settings.py dentro do arquivo bookspider.py (arquivo referente Ã  spider), basta adicionar  â€œcustom_settings = { 'FEEDS': { 'booksdata.json': {'format': 'json'}, 'overwrite': True }}â€ ao cÃ³digo
- Baixar e instalar SQLITE.
- Adicionar uma classe no arquivo pipelines.py para lidar com o banco [5.](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
- Adicionar um item no objeto ITEM_PIPELINES no arquivo settings.py com: "bookscraper.pipelines.SaveToSQLitePipeline": 400,"
</details>

<details>
<summary>Evitando bloqueio de requisiÃ§Ã£o de um site</summary>

- Criar mÃ©todo para adicionar header ao request [6.](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
- Adicionar variÃ¡veis que o mÃ©todo necessitar [6.1](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
- Adicionar middleware ao arquivo settings.py [6.2](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
</details>

<details>
<summary>Adicionando proxy Ã s requisiÃ§Ãµes</summary>

- Adicionar configuraÃ§Ã£o de proxy ao arquivo settings.py [7.](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
- Alternativa usando um serviÃ§o pago [7.1](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
- Alternativa usando scrapeops proxy api [7.2](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
</details>

<details>
<summary>Rodando uma Spider na nuvem </summary>[8.](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)

Criando um projeto Scrapy Cloud

---
- Criar uma conta em â€œ[https://app.zyte.com](https://app.zyte.com/o/599648)â€
- Ir atÃ© a aba Scrapy Cloud
- Criar um novo projeto clicando em â€œStart Projectâ€
- Em Scrapy Cloud, navegar no projeto criado
- Ir na sessÃ£o â€œSPIDERS/Code & Deploysâ€
- Clicar no botÃ£o â€œDeploy My codeâ€

Dando deploy do seu projeto

---
- No terminal do seu projeto instalar o â€œshubâ€ com o comando â€œpip install shubâ€
- Terminal: â€œshub login"
- Fornecer a chave de API apÃ³s â€œAPI key: SUA-CHAVE-APIâ€
- Executar Deploy com o comando â€œshub deploy PROJECT-IDâ€
- Aguardar execuÃ§Ã£o do deploy e verificar no Dashboard

Rodando o projeto

---

- No menu de navegaÃ§Ã£o do site do Scrapy Cloud, ir em â€œJOBS/Dashboardâ€
- Clicar em â€œRunâ€
- Selecionar a Spider que deseja rodar
- Clicar em â€œRunâ€

*Nota: Caso nÃ£o seja importado automaticamente, adicionar ao arquivo â€œscrapinghub.ymlâ€ a linha de cÃ³digo â€œrequirements_file: requirements.txtâ€ para que seja mapeado o arquivo de requirements.txt do projeto.* [Ver tutorial](https://www.notion.so/Page-3-Scrapy-Project-4aa86e19a54c459c9b5d4465e564ea92?pvs=21)
</details>
</aside>


<aside>
  
---
**Mais detalhes:**
<details>
<summary>Spider base criada a partir do comando â€œscrapy genspyder bookspider book.toscrape.comâ€</summary>
   
   ```python
    import scrapy
    
    
    class BookspiderSpider(scrapy.Spider):
        name = "bookspider"
        allowed_domains = ["books.toscrape.com"]
        start_urls = ["https://books.toscrape.com"]
    
        def parse(self, response):
            pass
   ```
</details>

## SeÃ§Ã£o 1
<details>
   <summary>  Alterando o mÃ©todo parse para retornar um objeto com atributos vindos da response</summary>
   ```python
    import scrapy


    class BookspiderSpider(scrapy.Spider):
        name = "bookspider"
        allowed_domains = ["books.toscrape.com"]
        start_urls = ["https://books.toscrape.com"]
    
        def parse(self, response):
            books = response.css('article.product_pod')
    
    #------------------------------------------------------------------------------------------------
            for book in books:
                yield{
                    'name': book.css('h3 a::text').get(),
                    'price': book.css('div p.price_color::text').get(),
                    'url': book.css('h3 a').attrib['href']
    
                }
   ```
</details>

<aside>
